{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwhbMsG1Kiwx",
        "outputId": "a9b57dd2-25c2-4fc7-cc88-8a88f4a0d614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-24 11:53:03--  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz\n",
            "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.31, 130.14.250.7, 130.14.250.10, ...\n",
            "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1379902 (1.3M) [application/x-gzip]\n",
            "Saving to: ‘GCF_000005845.2_ASM584v2_genomic.fna.gz’\n",
            "\n",
            "GCF_000005845.2_ASM 100%[===================>]   1.32M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-24 11:53:03 (13.5 MB/s) - ‘GCF_000005845.2_ASM584v2_genomic.fna.gz’ saved [1379902/1379902]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -hlrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo6hpUn8O-h9",
        "outputId": "21e8af4d-0bd3-40d5-a51e-140f4b0645d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1.4M\n",
            "-rw-r--r-- 1 root root 1.4M Oct 31  2014 GCF_000005845.2_ASM584v2_genomic.fna.gz\n",
            "drwxr-xr-x 1 root root 4.0K May 14 13:38 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d GCF_000005845.2_ASM584v2_genomic.fna.gz"
      ],
      "metadata": {
        "id": "PvpAtpJCO44j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -hlrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ck7j7ApO7uT",
        "outputId": "d370d287-5062-4033-98de-745f2a977215"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4.5M\n",
            "-rw-r--r-- 1 root root 4.5M Oct 31  2014 GCF_000005845.2_ASM584v2_genomic.fna\n",
            "drwxr-xr-x 1 root root 4.0K May 14 13:38 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head GCF_000005845.2_ASM584v2_genomic.fna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npRkBC3lPBqd",
        "outputId": "92b995c6-bc71-4f25-80b3-1649c820503c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">NC_000913.3 Escherichia coli str. K-12 substr. MG1655, complete genome\n",
            "AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGCTTCTGAACTG\n",
            "GTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC\n",
            "AGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCATTACCACAGGT\n",
            "AACGGTGCGGGCTGACGCGTACAGGAAACACAGAAAAAAGCCCGCACCTGACAGTGCGGGCTTTTTTTTTCGACCAAAGG\n",
            "TAACGAGGTAACAACCATGCGAGTGTTGAAGTTCGGCGGTACATCAGTGGCAAATGCAGAACGTTTTCTGCGTGTTGCCG\n",
            "ATATTCTGGAAAGCAATGCCAGGCAGGGGCAGGTGGCCACCGTCCTCTCTGCCCCCGCCAAAATCACCAACCACCTGGTG\n",
            "GCGATGATTGAAAAAACCATTAGCGGCCAGGATGCTTTACCCAATATCAGCGATGCCGAACGTATTTTTGCCGAACTTTT\n",
            "GACGGGACTCGCCGCCGCCCAGCCGGGGTTCCCGCTGGCGCAATTGAAAACTTTCGTCGATCAGGAATTTGCCCAAATAA\n",
            "AACATGTCCTGCATGGCATTAGTTTGTTGGGGCAGTGCCCGGATAGCATCAACGCTGCGCTGATTTGCCGTGGCGAGAAA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_fasta(filepath):\n",
        "    header = None\n",
        "    sequence_lines = []\n",
        "\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()  # Удаляем пробелы и символы новой строки\n",
        "            if not line:\n",
        "                continue  # Пропускаем пустые строки\n",
        "            if line.startswith('>'):\n",
        "                if header is not None:\n",
        "                    # Если уже есть заголовок, вернуть его и накопленную последовательность\n",
        "                    yield header, ''.join(sequence_lines)\n",
        "                header = line[1:]  # Убираем '>' в начале\n",
        "                sequence_lines = []  # Сброс буфера для новой последовательности\n",
        "            else:\n",
        "                sequence_lines.append(line)\n",
        "\n",
        "        # Возвращаем последнюю последовательность в файле\n",
        "        if header is not None:\n",
        "            yield header, ''.join(sequence_lines)\n",
        "\n",
        "fasta_path = \"GCF_000005845.2_ASM584v2_genomic.fna\"\n",
        "\n",
        "for header, sequence in parse_fasta(fasta_path):\n",
        "    print(\"Header:\", header)\n",
        "    print(\"Sequence:\", sequence[:60] + '...')  # Показать первые 60 символов"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19thxC8CPd4F",
        "outputId": "10421bf8-5ec8-410a-e9d7-315890e4750b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header: NC_000913.3 Escherichia coli str. K-12 substr. MG1655, complete genome\n",
            "Sequence: AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTC...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_complement(dna_seq):\n",
        "    complement = {\n",
        "        'A': 'T',\n",
        "        'T': 'A',\n",
        "        'G': 'C',\n",
        "        'C': 'G',\n",
        "        'a': 't',\n",
        "        't': 'a',\n",
        "        'g': 'c',\n",
        "        'c': 'g',\n",
        "        'N': 'N',\n",
        "        'n': 'n'\n",
        "    }\n",
        "    reversed_seq = dna_seq[::-1]\n",
        "    rev_comp = ''.join(complement.get(base, base) for base in reversed_seq)\n",
        "    return rev_comp"
      ],
      "metadata": {
        "id": "0z4uM1SaQqvE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUuIegbiPtCg",
        "outputId": "d9b5e607-c06d-4b49-88ef-b9d3abe05aec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4641652"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rev_comp_sequence = reverse_complement(sequence)"
      ],
      "metadata": {
        "id": "Xn5F4EjBQ5Sn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "methionine = \"ATG\"\n",
        "methionine_rc = \"CAT\"\n",
        "hits, hits_rc = sequence.count(methionine), sequence.count(methionine_rc)"
      ],
      "metadata": {
        "id": "DhnRY7ekPwYs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hits, hits_rc, hits + hits_rc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XyQdDMEQDvp",
        "outputId": "87cc2d85-fddf-4823-a56e-e4d400983b41"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(76282, 77041, 153323)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.gff.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pj94qTyRSmK",
        "outputId": "3616c570-7a6d-4214-f057-2b815c231da8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-24 11:53:19--  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.gff.gz\n",
            "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.31, 130.14.250.7, 130.14.250.10, ...\n",
            "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 433547 (423K) [application/x-gzip]\n",
            "Saving to: ‘GCF_000005845.2_ASM584v2_genomic.gff.gz’\n",
            "\n",
            "\r          GCF_00000   0%[                    ]       0  --.-KB/s               \rGCF_000005845.2_ASM 100%[===================>] 423.39K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-05-24 11:53:19 (5.76 MB/s) - ‘GCF_000005845.2_ASM584v2_genomic.gff.gz’ saved [433547/433547]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d GCF_000005845.2_ASM584v2_genomic.gff.gz"
      ],
      "metadata": {
        "id": "0n4tZPj_SB6g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head GCF_000005845.2_ASM584v2_genomic.gff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz2EAgUaSEo2",
        "outputId": "4d6074c2-8030-4ed9-95d6-86e1d6c583eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##gff-version 3\n",
            "#!gff-spec-version 1.21\n",
            "#!processor NCBI annotwriter\n",
            "#!genome-build ASM584v2\n",
            "#!genome-build-accession NCBI_Assembly:GCF_000005845.2\n",
            "##sequence-region NC_000913.3 1 4641652\n",
            "##species https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=511145\n",
            "NC_000913.3\tRefSeq\tregion\t1\t4641652\t.\t+\t.\tID=NC_000913.3:1..4641652;Dbxref=taxon:511145;Is_circular=true;Name=ANONYMOUS;gbkey=Src;genome=chromosome;mol_type=genomic DNA;strain=K-12;substrain=MG1655\n",
            "NC_000913.3\tRefSeq\tgene\t190\t255\t.\t+\t.\tID=gene-b0001;Dbxref=ASAP:ABE-0000006,ECOCYC:EG11277,GeneID:944742;Name=thrL;gbkey=Gene;gene=thrL;gene_biotype=protein_coding;gene_synonym=ECK0001;locus_tag=b0001\n",
            "NC_000913.3\tRefSeq\tCDS\t190\t255\t.\t+\t0\tID=cds-NP_414542.1;Parent=gene-b0001;Dbxref=UniProtKB/Swiss-Prot:P0AD86,GenBank:NP_414542.1,ASAP:ABE-0000006,ECOCYC:EG11277,GeneID:944742;Name=NP_414542.1;gbkey=CDS;gene=thrL;locus_tag=b0001;orig_transcript_id=gnl|b0001|mrna.NP_414542;product=thr operon leader peptide;protein_id=NP_414542.1;transl_table=11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail !head GCF_000005845.2_ASM584v2_genomic.gff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeqDA5QgSJVI",
        "outputId": "f488c65c-acd2-4214-dd43-cfeb9eabd7d7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tail: cannot open '!head' for reading: No such file or directory\n",
            "==> GCF_000005845.2_ASM584v2_genomic.gff <==\n",
            "NC_000913.3\tRefSeq\tCDS\t4638178\t4639530\t.\t+\t0\tID=cds-NP_418817.1;Parent=gene-b4400;Dbxref=UniProtKB/Swiss-Prot:P08369,GenBank:NP_418817.1,ASAP:ABE-0014432,ECOCYC:EG10145,GeneID:948868;Name=NP_418817.1;gbkey=CDS;gene=creD;locus_tag=b4400;orig_transcript_id=gnl|b4400|mrna.NP_418817;product=putative inner membrane protein CreD;protein_id=NP_418817.1;transl_table=11\n",
            "NC_000913.3\tRefSeq\tgene\t4639590\t4640306\t.\t-\t.\tID=gene-b4401;Dbxref=ASAP:ABE-0014434,ECOCYC:EG10061,GeneID:948874;Name=arcA;gbkey=Gene;gene=arcA;gene_biotype=protein_coding;gene_synonym=cpxC,dye,ECK4393,fexA,msp,seg,sfrA;locus_tag=b4401\n",
            "NC_000913.3\tRefSeq\tCDS\t4639590\t4640306\t.\t-\t0\tID=cds-NP_418818.1;Parent=gene-b4401;Dbxref=UniProtKB/Swiss-Prot:P0A9Q1,GenBank:NP_418818.1,ASAP:ABE-0014434,ECOCYC:EG10061,GeneID:948874;Name=NP_418818.1;gbkey=CDS;gene=arcA;locus_tag=b4401;orig_transcript_id=gnl|b4401|mrna.NP_418818;product=DNA-binding transcriptional dual regulator ArcA;protein_id=NP_418818.1;transl_table=11\n",
            "NC_000913.3\tRefSeq\tgene\t4640402\t4640542\t.\t+\t.\tID=gene-b4402;Dbxref=ASAP:ABE-0014437,ECOCYC:G7954,GeneID:948925;Name=yjjY;gbkey=Gene;gene=yjjY;gene_biotype=protein_coding;gene_synonym=ECK4394;locus_tag=b4402\n",
            "NC_000913.3\tRefSeq\tCDS\t4640402\t4640542\t.\t+\t0\tID=cds-NP_418819.1;Parent=gene-b4402;Dbxref=UniProtKB/Swiss-Prot:P0ADD9,GenBank:NP_418819.1,ASAP:ABE-0014437,ECOCYC:G7954,GeneID:948925;Name=NP_418819.1;gbkey=CDS;gene=yjjY;locus_tag=b4402;orig_transcript_id=gnl|b4402|mrna.NP_418819;product=uncharacterized protein YjjY;protein_id=NP_418819.1;transl_table=11\n",
            "NC_000913.3\tRefSeq\tgene\t4640718\t4640771\t.\t+\t.\tID=gene-b4824;Dbxref=ECOCYC:G0-17098,GeneID:71004582;Name=ytjE;gbkey=Gene;gene=ytjE;gene_biotype=protein_coding;gene_synonym=ECK4681;locus_tag=b4824\n",
            "NC_000913.3\tRefSeq\tCDS\t4640718\t4640771\t.\t+\t0\tID=cds-YP_010283921.1;Parent=gene-b4824;Dbxref=UniProtKB/Swiss-Prot:P0DV23,GenBank:YP_010283921.1,ECOCYC:G0-17098,GeneID:71004582;Name=YP_010283921.1;gbkey=CDS;gene=ytjE;locus_tag=b4824;orig_transcript_id=gnl|b4824|mrna.CDS4637;product=protein YtjE;protein_id=YP_010283921.1;transl_table=11\n",
            "NC_000913.3\tRefSeq\tgene\t4640942\t4641628\t.\t+\t.\tID=gene-b4403;Dbxref=ASAP:ABE-0014442,ECOCYC:EG12309,GeneID:948924;Name=yjtD;gbkey=Gene;gene=yjtD;gene_biotype=protein_coding;gene_synonym=ECK4395,lasT;locus_tag=b4403\n",
            "NC_000913.3\tRefSeq\tCDS\t4640942\t4641628\t.\t+\t0\tID=cds-NP_418820.1;Parent=gene-b4403;Dbxref=UniProtKB/Swiss-Prot:P37005,GenBank:NP_418820.1,ASAP:ABE-0014442,ECOCYC:EG12309,GeneID:948924;Name=NP_418820.1;gbkey=CDS;gene=yjtD;locus_tag=b4403;orig_transcript_id=gnl|b4403|mrna.NP_418820;product=putative tRNA/rRNA methyltransferase YjtD;protein_id=NP_418820.1;transl_table=11\n",
            "###\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_gff(filepath):\n",
        "    annotations = []\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith('#'):\n",
        "                continue  # Пропускаем комментарии и пустые строки\n",
        "\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) != 9:\n",
        "                continue  # Пропускаем некорректные строки\n",
        "\n",
        "            seqid, source, feature_type, start, end, score, strand, phase, attributes = parts\n",
        "\n",
        "            # Парсим колонку attributes в словарь\n",
        "            attr_dict = {}\n",
        "            for attr in attributes.split(';'):\n",
        "                if '=' in attr:\n",
        "                    key, value = attr.split('=', 1)\n",
        "                    attr_dict[key] = value\n",
        "\n",
        "            annotations.append({\n",
        "                'seqid': seqid,\n",
        "                'source': source,\n",
        "                'type': feature_type,\n",
        "                'start': int(start),\n",
        "                'end': int(end),\n",
        "                'score': score if score != '.' else None,\n",
        "                'strand': strand,\n",
        "                'phase': phase if phase != '.' else None,\n",
        "                'attributes': attr_dict\n",
        "            })\n",
        "\n",
        "    return annotations"
      ],
      "metadata": {
        "id": "mLUO90DQSLDO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations = parse_gff(\"GCF_000005845.2_ASM584v2_genomic.gff\")"
      ],
      "metadata": {
        "id": "8LV6g6qpSuKp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDUjxgLFS0Ce",
        "outputId": "2270b3ec-8ae7-4015-c3d4-83a34449395d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'seqid': 'NC_000913.3',\n",
              "  'source': 'RefSeq',\n",
              "  'type': 'region',\n",
              "  'start': 1,\n",
              "  'end': 4641652,\n",
              "  'score': None,\n",
              "  'strand': '+',\n",
              "  'phase': None,\n",
              "  'attributes': {'ID': 'NC_000913.3:1..4641652',\n",
              "   'Dbxref': 'taxon:511145',\n",
              "   'Is_circular': 'true',\n",
              "   'Name': 'ANONYMOUS',\n",
              "   'gbkey': 'Src',\n",
              "   'genome': 'chromosome',\n",
              "   'mol_type': 'genomic DNA',\n",
              "   'strain': 'K-12',\n",
              "   'substrain': 'MG1655'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_gff(features):\n",
        "    for i, feature in enumerate(features, 1):\n",
        "        print(f\"Feature {i}:\")\n",
        "        print(f\"  Type     : {feature['type']}\")\n",
        "        print(f\"  Location : {feature['seqid']}:{feature['start']}..{feature['end']} ({feature['strand']})\")\n",
        "        if feature['score'] is not None:\n",
        "            print(f\"  Score    : {feature['score']}\")\n",
        "        if feature['phase'] is not None:\n",
        "            print(f\"  Phase    : {feature['phase']}\")\n",
        "        print(f\"  Source   : {feature['source']}\")\n",
        "        if feature['attributes']:\n",
        "            print(f\"  Attributes:\")\n",
        "            for key, value in feature['attributes'].items():\n",
        "                print(f\"    - {key}: {value}\")\n",
        "        print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "Qch_p_8pS2j0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretty_print_gff(annotations[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIaKAAifTFlL",
        "outputId": "39f64dc5-b7aa-44d0-d901-3d1f1ac5083a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 1:\n",
            "  Type     : region\n",
            "  Location : NC_000913.3:1..4641652 (+)\n",
            "  Source   : RefSeq\n",
            "  Attributes:\n",
            "    - ID: NC_000913.3:1..4641652\n",
            "    - Dbxref: taxon:511145\n",
            "    - Is_circular: true\n",
            "    - Name: ANONYMOUS\n",
            "    - gbkey: Src\n",
            "    - genome: chromosome\n",
            "    - mol_type: genomic DNA\n",
            "    - strain: K-12\n",
            "    - substrain: MG1655\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def parse_gff(filepath):\n",
        "    annotations = []\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith('#'):\n",
        "                continue\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) != 9:\n",
        "                continue\n",
        "            seqid, source, feature_type, start, end, score, strand, phase, attributes = parts\n",
        "            attr_dict = {}\n",
        "            for attr in attributes.split(';'):\n",
        "                if '=' in attr:\n",
        "                    key, value = attr.split('=', 1)\n",
        "                    attr_dict[key] = value\n",
        "            annotations.append({\n",
        "                'seqid': seqid,\n",
        "                'source': source,\n",
        "                'type': feature_type,\n",
        "                'start': int(start),\n",
        "                'end': int(end),\n",
        "                'score': score if score != '.' else None,\n",
        "                'strand': strand,\n",
        "                'phase': phase if phase != '.' else None,\n",
        "                'attributes': attr_dict\n",
        "            })\n",
        "    return annotations\n",
        "\n",
        "def plot_gene_lengths(gff_file):\n",
        "    features = parse_gff(gff_file)\n",
        "    genes = [f for f in features if f['type'] == 'gene']\n",
        "    gene_lengths = [gene['end'] - gene['start'] + 1 for gene in genes]\n",
        "    gene_ids = [gene['attributes'].get('ID', 'unknown') for gene in genes]\n",
        "\n",
        "    # Находим самый короткий и самый длинный ген\n",
        "    min_index = gene_lengths.index(min(gene_lengths))\n",
        "    max_index = gene_lengths.index(max(gene_lengths))\n",
        "    min_gene = {'id': gene_ids[min_index], 'length': gene_lengths[min_index]}\n",
        "    max_gene = {'id': gene_ids[max_index], 'length': gene_lengths[max_index]}\n",
        "\n",
        "    # Строим гистограмму\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(gene_lengths, bins=50, edgecolor='black')\n",
        "    plt.title(\"Distribution of Gene Lengths\")\n",
        "    plt.xlabel(\"Gene Length (bp)\")\n",
        "    plt.ylabel(\"Number of Genes\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Выводим информацию\n",
        "    print(f\"Shortest gene: {min_gene['id']} ({min_gene['length']} bp)\")\n",
        "    print(f\"Longest gene:  {max_gene['id']} ({max_gene['length']} bp)\")\n",
        "\n",
        "\n",
        "def plot_gene_lengths(gff_file):\n",
        "    features = parse_gff(gff_file)\n",
        "    genes = [f for f in features if f['type'] == 'gene']\n",
        "    gene_lengths = [gene['end'] - gene['start'] + 1 for gene in genes]\n",
        "\n",
        "    # Получаем данные для подписи\n",
        "    def get_gene_info(gene):\n",
        "        attrs = gene['attributes']\n",
        "        return {\n",
        "            'id': attrs.get('ID', 'unknown'),\n",
        "            'gene': attrs.get('gene', '-'),\n",
        "            'product': attrs.get('product', '-'),\n",
        "            'length': gene['end'] - gene['start'] + 1\n",
        "        }\n",
        "\n",
        "    # Находим самый короткий и самый длинный\n",
        "    min_index = gene_lengths.index(min(gene_lengths))\n",
        "    max_index = gene_lengths.index(max(gene_lengths))\n",
        "    min_gene = get_gene_info(genes[min_index])\n",
        "    max_gene = get_gene_info(genes[max_index])\n",
        "\n",
        "    # Строим гистограмму\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(gene_lengths, bins=50, edgecolor='black')\n",
        "    plt.title(\"Distribution of Gene Lengths\")\n",
        "    plt.xlabel(\"Gene Length (bp)\")\n",
        "    plt.ylabel(\"Number of Genes\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Печатаем результат\n",
        "    print(f\"Shortest gene:\")\n",
        "    print(f\"  ID      : {min_gene['id']}\")\n",
        "    print(f\"  Name    : {min_gene['gene']}\")\n",
        "    print(f\"  Product : {min_gene['product']}\")\n",
        "    print(f\"  Length  : {min_gene['length']} bp\")\n",
        "\n",
        "    print(f\"\\nLongest gene:\")\n",
        "    print(f\"  ID      : {max_gene['id']}\")\n",
        "    print(f\"  Name    : {max_gene['gene']}\")\n",
        "    print(f\"  Product : {max_gene['product']}\")\n",
        "    print(f\"  Length  : {max_gene['length']} bp\")\n",
        "\n",
        "# Пример запуска\n",
        "gff_path = \"GCF_000005845.2_ASM584v2_genomic.gff\"  # замените на путь к вашему GFF-файлу\n",
        "plot_gene_lengths(gff_path)"
      ],
      "metadata": {
        "id": "YjjnYCGhTKxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/914/755/GCF_009914755.1_T2T-CHM13v2.0/GCF_009914755.1_T2T-CHM13v2.0_genomic.gff.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcKKUkaUUXcy",
        "outputId": "cf34c85d-a40f-4f60-cf9c-bb7d2fa88af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-22 09:35:59--  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/009/914/755/GCF_009914755.1_T2T-CHM13v2.0/GCF_009914755.1_T2T-CHM13v2.0_genomic.gff.gz\n",
            "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.13, 130.14.250.31, 130.14.250.7, ...\n",
            "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 79063589 (75M) [application/x-gzip]\n",
            "Saving to: ‘GCF_009914755.1_T2T-CHM13v2.0_genomic.gff.gz’\n",
            "\n",
            "GCF_009914755.1_T2T 100%[===================>]  75.40M  20.4MB/s    in 4.9s    \n",
            "\n",
            "2025-05-22 09:36:05 (15.4 MB/s) - ‘GCF_009914755.1_T2T-CHM13v2.0_genomic.gff.gz’ saved [79063589/79063589]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d GCF_009914755.1_T2T-CHM13v2.0_genomic.gff.gz"
      ],
      "metadata": {
        "id": "voHkKVitWX4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -hlrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElQNjcJ3Wl61",
        "outputId": "9e4efe29-40c5-4845-c38b-a213062dc752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1.6G\n",
            "-rw-r--r-- 1 root root 4.5M Oct 31  2014  GCF_000005845.2_ASM584v2_genomic.fna\n",
            "-rw-r--r-- 1 root root 2.6M Jun 18  2023  GCF_000005845.2_ASM584v2_genomic.gff\n",
            "-rw-r--r-- 1 root root 1.6G Aug 27  2024  GCF_009914755.1_T2T-CHM13v2.0_genomic.gff\n",
            "drwxr-xr-x 1 root root 4.0K May 14 13:38  sample_data\n",
            "-rw-r--r-- 1 root root 8.2K May 22 09:03 '83338241991?pwd=4Olrrp0PMbmu5hroMecZ3BdsyvESZi.1'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gff_path_human = \"GCF_009914755.1_T2T-CHM13v2.0_genomic.gff\"  # замените на путь к вашему GFF-файлу\n",
        "plot_gene_lengths(gff_path_human)"
      ],
      "metadata": {
        "id": "4jvtC4rqWe7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.gff.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "366qQ3P4Wt6L",
        "outputId": "c7ac58f1-01fc-4c41-d826-0766466d7809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-22 09:42:31--  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.gff.gz\n",
            "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.12, 130.14.250.13, 130.14.250.31, ...\n",
            "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77807126 (74M) [application/x-gzip]\n",
            "Saving to: ‘GCF_000001405.40_GRCh38.p14_genomic.gff.gz’\n",
            "\n",
            "GCF_000001405.40_GR 100%[===================>]  74.20M  18.0MB/s    in 5.5s    \n",
            "\n",
            "2025-05-22 09:42:38 (13.4 MB/s) - ‘GCF_000001405.40_GRCh38.p14_genomic.gff.gz’ saved [77807126/77807126]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d GCF_000001405.40_GRCh38.p14_genomic.gff.gz"
      ],
      "metadata": {
        "id": "rUniFWacX3gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gff_path_human_ref = \"GCF_000001405.40_GRCh38.p14_genomic.gff\"  # замените на путь к вашему GFF-файлу\n",
        "plot_gene_lengths(gff_path_human_ref)"
      ],
      "metadata": {
        "id": "9Ub1xuC4X5x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_orfs(sequence):\n",
        "    start_codon = 'ATG'\n",
        "    stop_codons = {'TAA', 'TAG', 'TGA'}\n",
        "    sequence = sequence.upper()\n",
        "    orfs = []\n",
        "\n",
        "    for frame in range(3):  # три возможных рамки считывания\n",
        "        i = frame\n",
        "        while i + 3 <= len(sequence):\n",
        "            codon = sequence[i:i+3]\n",
        "            if codon == start_codon:\n",
        "                # нашли старт, начинаем искать стоп\n",
        "                j = i + 3\n",
        "                while j + 3 <= len(sequence):\n",
        "                    next_codon = sequence[j:j+3]\n",
        "                    if next_codon in stop_codons:\n",
        "                        orf_seq = sequence[i:j+3]  # включая стоп-кодон\n",
        "                        orfs.append(orf_seq)\n",
        "                        break  # нашли ORF — выходим, ищем следующий старт\n",
        "                    j += 3\n",
        "                i = j  # можно прыгнуть к стопу\n",
        "            else:\n",
        "                i += 3\n",
        "    return orfs"
      ],
      "metadata": {
        "id": "kvGb3aKaX9yD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_orfs_nn(sequence):\n",
        "    sequence = sequence.upper()\n",
        "    stop_codons = {\"TAA\", \"TAG\", \"TGA\"}\n",
        "    orfs = []\n",
        "\n",
        "    # Проверка по трем фреймам\n",
        "    for frame in range(3):\n",
        "        i = frame\n",
        "        in_orf = False\n",
        "        while i + 3 <= len(sequence):\n",
        "            codon = sequence[i:i+3]\n",
        "            if not in_orf:\n",
        "                if codon == \"ATG\":\n",
        "                    start = i\n",
        "                    in_orf = True\n",
        "            else:\n",
        "                if codon in stop_codons:\n",
        "                    end = i + 3\n",
        "                    orfs.append((start, end, frame))\n",
        "                    i = end - 3  # Продолжим поиск после стоп-кодона\n",
        "                    in_orf = False\n",
        "            i += 3\n",
        "    return orfs\n",
        "\n",
        "# Пример\n",
        "if __name__ == \"__main__\":\n",
        "    genome = \"TTGATGAAATGAAAAGTAGATGTTTTAGATGTAA\"\n",
        "    orfs = find_orfs_nn(genome)\n",
        "    for start, end, frame in orfs:\n",
        "        print(f\"ORF: {start}-{end} (frame {frame}) => {genome[start:end]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNmT7sQka1WI",
        "outputId": "b8cce294-3365-4a80-b030-1fd45250198f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORF: 3-12 (frame 0) => ATGAAATGA\n",
            "ORF: 19-28 (frame 1) => ATGTTTTAG\n",
            "ORF: 28-34 (frame 1) => ATGTAA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orfs_plus = find_orfs(sequence)\n",
        "orfs_minus = find_orfs(reverse_complement(sequence))"
      ],
      "metadata": {
        "id": "xVcCadMmZLkH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(orfs_plus), len(orfs_minus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI93kCsmZOGx",
        "outputId": "f5d77a4f-2a48-4bbe-a819-34f11ce48a4e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40954, 40733)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orfs_plus_nn = find_orfs_nn(sequence)\n",
        "orfs_minus_nn = find_orfs_nn(reverse_complement(sequence))\n",
        "len(orfs_plus_nn), len(orfs_minus_nn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DBzFN8ja88E",
        "outputId": "e7a33848-f006-47b2-c1bc-f642c19d6028"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40954, 40733)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orfs_plus[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9RWdXYyzZRJe",
        "outputId": "21be1b3f-a541-4073-9b87-82692ce15b57"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ATGCGACGCCGGTCGCGTCTTATCCGGCCTTCCTATATCAGGCTGTGTTTAAGACGCCGCCGCTTCGCCCAAATCCTTATGCCGGTTCGACGGCTGGACAAAATACTGTTTATCTTCCCAGCGCAGGCAGGTTAA'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orfs_plus_nn[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twKstTRaZlVC",
        "outputId": "2702a0d5-4b17-48e2-a671-81e64a268eec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50331, 50466, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHuvaaxNbNAm",
        "outputId": "72b51983-58f6-47ed-b041-c29e009741ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-22 10:09:42--  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\n",
            "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.7, 130.14.250.10, 130.14.250.12, ...\n",
            "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 972898531 (928M) [application/x-gzip]\n",
            "Saving to: ‘GCF_000001405.40_GRCh38.p14_genomic.fna.gz’\n",
            "\n",
            "GCF_000001405.40_GR 100%[===================>] 927.83M  9.76MB/s    in 2m 27s  \n",
            "\n",
            "2025-05-22 10:12:10 (6.29 MB/s) - ‘GCF_000001405.40_GRCh38.p14_genomic.fna.gz’ saved [972898531/972898531]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d GCF_000001405.40_GRCh38.p14_genomic.gff.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQAnuE3fd5n8",
        "outputId": "fb550602-db72-4f17-86bb-89cee39e723f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzip: GCF_000001405.40_GRCh38.p14_genomic.gff already exists; do you wish to overwrite (y or n)? ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_gff(file_path):\n",
        "    genes = defaultdict(list)\n",
        "    with open(file_path, 'r') as file:\n",
        "        reader = csv.reader(file, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            if row[0].startswith('#') or row[2] != 'gene':\n",
        "                continue\n",
        "            chrom = row[0]\n",
        "            start = int(row[3])\n",
        "            end = int(row[4])\n",
        "            strand = row[6]\n",
        "            genes[chrom].append((start, end, strand))\n",
        "    return genes\n",
        "\n",
        "def find_overlaps(genes):\n",
        "    overlaps = []\n",
        "    for chrom in genes:\n",
        "        sorted_genes = sorted(genes[chrom], key=lambda x: x[0])\n",
        "        for i in range(len(sorted_genes)):\n",
        "            for j in range(i + 1, len(sorted_genes)):\n",
        "                gene1 = sorted_genes[i]\n",
        "                gene2 = sorted_genes[j]\n",
        "                if gene2[0] <= gene1[1]:\n",
        "                    overlaps.append((chrom, gene1, gene2))\n",
        "                else:\n",
        "                    break\n",
        "    return overlaps\n",
        "\n",
        "# Пример использования\n",
        "gff_file = 'GCF_000005845.2_ASM584v2_genomic.gff'\n",
        "genes = parse_gff(gff_file)\n",
        "overlapping_genes = find_overlaps(genes)\n",
        "\n",
        "len(overlapping_genes)\n",
        "# for overlap in overlapping_genes:\n",
        "#     chrom, gene1, gene2 = overlap\n",
        "#     print(f'Overlap on {chrom}: {gene1} and {gene2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fJWCRqZeAuB",
        "outputId": "48f3d4c3-f903-4a58-b546-85ae0ea4e2c2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "def find_orfs_bacterial(dna_sequence: str) -> list:\n",
        "    \"\"\"\n",
        "    Находит открытые рамки считывания (ОРФ) в геномной последовательности бактерии.\n",
        "\n",
        "    Концепция:\n",
        "    1. ОРФы начинаются с кодона метионина (ATG).\n",
        "    2. ОРФы должны находиться в одной рамке считывания.\n",
        "    3. ОРФы заканчиваются первым встреченным стоп-кодоном (TAA, TAG, TGA)\n",
        "       в той же рамке считывания.\n",
        "    4. Вложенные метионины (ATG), находящиеся внутри уже идентифицированного ОРФа\n",
        "       и в той же рамке считывания, не начинают новый ОРФ.\n",
        "    5. Вложенные метионины, но в другой рамке считывания, могут начинать новый ОРФ.\n",
        "\n",
        "    Args:\n",
        "        dna_sequence (str): Строка ДНК, состоящая из символов 'A', 'T', 'G', 'C'.\n",
        "                            Регистр будет приведен к верхнему.\n",
        "\n",
        "    Returns:\n",
        "        list: Список словарей, где каждый словарь представляет ОРФ и содержит:\n",
        "              'start': начальный индекс ОРФа (0-based) в исходной последовательности.\n",
        "              'end': конечный индекс ОРФа (0-based, не включая сам конечный индекс,\n",
        "                     т.е. подходит для среза dna_sequence[start:end]).\n",
        "              'frame': рамка считывания (1, 2 или 3).\n",
        "              'sequence': последовательность ДНК ОРФа (включая старт- и стоп-кодоны).\n",
        "              'length': длина ОРФа в нуклеотидах.\n",
        "    \"\"\"\n",
        "    if not dna_sequence:\n",
        "        return []\n",
        "\n",
        "    dna_sequence = dna_sequence.upper()\n",
        "    n = len(dna_sequence)\n",
        "\n",
        "    start_codon = \"ATG\"\n",
        "    stop_codons = [\"TAA\", \"TAG\", \"TGA\"]\n",
        "\n",
        "    found_orfs = []\n",
        "\n",
        "    # Словарь для хранения уже идентифицированных регионов ОРФ для каждой рамки.\n",
        "    # Ключ: рамка (0, 1, 2), Значение: список кортежей (start_abs_index, end_abs_index)\n",
        "    # defaultdict используется для удобства, чтобы не проверять наличие ключа перед добавлением.\n",
        "    identified_orf_regions_by_frame = collections.defaultdict(list)\n",
        "\n",
        "    # Итерируем по трем возможным рамкам считывания (0, 1, 2)\n",
        "    for frame_offset in range(3):\n",
        "\n",
        "        # 1. Находим все потенциальные стартовые кодоны (ATG) в текущей рамке\n",
        "        potential_atg_indices_in_frame = []\n",
        "        # Идем по последовательности с шагом 3, начиная с текущего смещения рамки (frame_offset)\n",
        "        # n - 2 гарантирует, что мы не выйдем за пределы строки при взятии кодона dna_sequence[i : i + 3]\n",
        "        for i in range(frame_offset, n - 2, 3):\n",
        "            codon = dna_sequence[i : i + 3]\n",
        "            if codon == start_codon:\n",
        "                potential_atg_indices_in_frame.append(i)\n",
        "\n",
        "        # 2. Для каждого потенциального ATG ищем ОРФ\n",
        "        for atg_start_index in potential_atg_indices_in_frame:\n",
        "\n",
        "            # Проверка на вложенность в той же рамке считывания:\n",
        "            # Метионин (atg_start_index) считается вложенным, если его начальная позиция\n",
        "            # находится внутри уже найденного ОРФа (orf_s <= atg_start_index < orf_e).\n",
        "            is_nested_in_same_frame = False\n",
        "            for orf_s, orf_e in identified_orf_regions_by_frame[frame_offset]:\n",
        "                if orf_s <= atg_start_index < orf_e:\n",
        "                    is_nested_in_same_frame = True\n",
        "                    break # Нашли вложение, дальше проверять не нужно\n",
        "\n",
        "            if is_nested_in_same_frame:\n",
        "                continue # Этот ATG вложен в ОРФ в той же рамке, пропускаем его\n",
        "\n",
        "            # Ищем первый стоп-кодон, начиная со следующего кодона после ATG\n",
        "            # Поиск также идет с шагом 3, в той же рамке считывания\n",
        "            for j in range(atg_start_index + 3, n - 2, 3):\n",
        "                codon = dna_sequence[j : j + 3]\n",
        "                if codon in stop_codons:\n",
        "                    # Стоп-кодон найден. Определяем конец ОРФа.\n",
        "                    orf_end_index = j + 3 # Конечный индекс для среза (не включая сам этот индекс)\n",
        "\n",
        "                    orf_info = {\n",
        "                        \"start\": atg_start_index, # 0-based начальный индекс\n",
        "                        \"end\": orf_end_index,     # 0-based конечный индекс (для среза)\n",
        "                        \"frame\": frame_offset + 1, # Рамка 1, 2 или 3 для пользователя\n",
        "                        \"sequence\": dna_sequence[atg_start_index:orf_end_index],\n",
        "                        \"length\": orf_end_index - atg_start_index\n",
        "                    }\n",
        "                    found_orfs.append(orf_info)\n",
        "\n",
        "                    # Запоминаем регион этого ОРФа для текущей рамки,\n",
        "                    # чтобы корректно обрабатывать последующие ATG.\n",
        "                    identified_orf_regions_by_frame[frame_offset].append((atg_start_index, orf_end_index))\n",
        "\n",
        "                    # ОРФ заканчивается первым встреченным стоп-кодоном,\n",
        "                    # поэтому прекращаем поиск стоп-кодонов для текущего ATG.\n",
        "                    break\n",
        "\n",
        "            # Если цикл по j завершился без break, значит стоп-кодон не был найден\n",
        "            # для текущего ATG до конца последовательности в этой рамке. Такой ОРФ неполный и не добавляется.\n",
        "\n",
        "    # Опционально: отсортировать найденные ОРФы, например, по стартовой позиции и рамке.\n",
        "    found_orfs.sort(key=lambda x: (x['start'], x['frame']))\n",
        "\n",
        "    return found_orfs"
      ],
      "metadata": {
        "id": "inolCTY1hQLX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orf_plus_nn = find_orfs_bacterial(sequence)\n",
        "orf_minus_nn = find_orfs_bacterial(reverse_complement(sequence))\n",
        "total_orfs = orf_plus_nn + orf_minus_nn\n",
        "len(orf_plus_nn), len(orf_minus_nn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW508d5xkApD",
        "outputId": "845ee19c-aa17-4bae-ebe8-ce582e294bb1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40954, 40733)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in orf_minus_nn[100:105]:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGzuB6S5nmxy",
        "outputId": "77839d7f-1b66-49a9-f557-e6ab96fcba4c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'start': 9810, 'end': 9858, 'frame': 1, 'sequence': 'ATGCAAAATCTCTTTTGCTTCAGCTTCACGTCCGCGTTCCAGCAATAA', 'length': 48}\n",
            "{'start': 10003, 'end': 10087, 'frame': 2, 'sequence': 'ATGGCGTCATCGCGCCATTTCGCCTGCTCGTCGGTGACATCGTTGCCCATCAAACGCCAGGCGACGATATCGCGCAGCTCCTGA', 'length': 84}\n",
            "{'start': 10124, 'end': 10202, 'frame': 3, 'sequence': 'ATGGGATCATCAGCCGTGCATTCTCAGCATCCTGCCGCGCCACACTGGCAAACGCCACCGCCGCCATTTGACGGGTAA', 'length': 78}\n",
            "{'start': 10264, 'end': 10291, 'frame': 2, 'sequence': 'ATGATTGCCGAGGCGATAGTCTGGTAA', 'length': 27}\n",
            "{'start': 10949, 'end': 10970, 'frame': 3, 'sequence': 'ATGCAGCGTCATGCCGCGTAA', 'length': 21}\n",
            "{'start': 10959, 'end': 11043, 'frame': 1, 'sequence': 'ATGCCGCGTAATAGATTTATGCTAACCAGTCATTGCCGTTTACGCCACGTTACGGACACTTTTTTACTTTTACTGCGAGGGTGA', 'length': 84}\n",
            "{'start': 11151, 'end': 12798, 'frame': 1, 'sequence': 'ATGCATCGTGTCGGCAAAGTTGTTCCGCCGAAACGTCATATTTTGAAAAACATCTCTCTGAGTTTCTTCCCTGGGGCAAAAATTGGTGTCCTGGGTCTGAATGGCGCGGGTAAGTCCACCCTGCTGCGCATTATGGCGGGCATTGATAAAGACATCGAAGGTGAAGCGCGTCCGCAGCCAGACATCAAGATTGGTTATCTGCCGCAGGAACCGCAGCTGAACCCGGAACACACCGTGCGTGAGTCCATTGAAGAAGCGGTTTCAGAAGTGGTTAACGCCCTGAAACGCCTGGATGAAGTGTATGCGCTGTACGCCGATCCGGATGCCGATTTTGACAAGCTGGCCGCTGAACAAGGCCGTCTGGAAGAGATCATTCAGGCTCACGACGGTCATAATCTGAACGTACAGCTGGAGCGTGCGGCGGATGCGCTACGTCTGCCGGACTGGGACGCGAAAATCGCTAACCTCTCCGGTGGTGAACGTCGTCGCGTAGCGTTGTGCCGCCTGCTGCTGGAAAAACCAGACATGCTGCTGCTCGACGAACCGACCAACCACCTGGATGCCGAATCCGTGGCCTGGCTGGAACGCTTCCTGCACGACTTCGAAGGCACCGTTGTGGCGATTACCCACGACCGTTACTTCCTCGATAACGTTGCAGGCTGGATCCTCGAACTTGACCGCGGTGAAGGTATTCCGTGGGAAGGTAACTACTCCTCCTGGCTGGAGCAGAAAGATCAGCGCCTGGCGCAGGAAGCTTCACAAGAAGCGGCGCGTCGTAAGTCGATTGAGAAAGAGCTGGAATGGGTACGTCAAGGTACTAAAGGCCGTCAGTCGAAAGGTAAAGCACGTCTGGCGCGCTTTGAAGAACTGAACAGCACCGAATATCAGAAACGTAACGAAACCAACGAACTGTTTATTCCACCTGGACCGCGTCTGGGCGATAAAGTGCTGGAAGTCAGCAACCTGCGTAAATCCTATGGCGATCGTCTGCTGATTGATGACCTGAGCTTCTCGATCCCGAAAGGAGCGATCGTCGGGATCATCGGTCCGAACGGTGCGGGTAAATCGACCCTGTTCCGTATGATCTCTGGTCAGGAACAGCCGGACAGCGGCACCATCACTTTGGGTGAAACGGTGAAACTGGCGTCGGTTGATCAGTTCCGTGACTCAATGGATAACAGCAAAACCGTTTGGGAAGAAGTTTCCGGCGGGCTGGATATCATGAAGATCGGCAACACCGAGATGCCAAGCCGCGCCTACGTTGGCCGCTTTAACTTTAAAGGGGTTGATCAGGGTAAACGCGTTGGTGAACTCTCCGGTGGTGAGCGCGGTCGTCTGCATCTGGCGAAGCTGCTGCAGGTTGGCGGCAACATGCTGCTGCTCGACGAACCAACCAACGACCTGGATATCGAAACCCTGCGCGCGCTGGAAAACGCCCTGCTGGAGTTCCCGGGCTGTGCGATGGTTATCTCGCACGACCGTTGGTTCCTCGACCGTATCGCCACGCACATTCTGGATTACCAGGATGAAGGTAAAGTTGAGTTCTTCGAAGGTAACTTTACCGAGTACGAAGAGTACAAGAAACGCACGCTGGGCGCAGACGCGCTGGAGCCGAAGCGTATCAAGTACAAGCGTATTGCGAAGTAA', 'length': 1647}\n",
            "{'start': 11251, 'end': 11371, 'frame': 2, 'sequence': 'ATGGCGCGGGTAAGTCCACCCTGCTGCGCATTATGGCGGGCATTGATAAAGACATCGAAGGTGAAGCGCGTCCGCAGCCAGACATCAAGATTGGTTATCTGCCGCAGGAACCGCAGCTGA', 'length': 120}\n",
            "{'start': 11443, 'end': 11551, 'frame': 2, 'sequence': 'ATGAAGTGTATGCGCTGTACGCCGATCCGGATGCCGATTTTGACAAGCTGGCCGCTGAACAAGGCCGTCTGGAAGAGATCATTCAGGCTCACGACGGTCATAATCTGA', 'length': 108}\n",
            "{'start': 11575, 'end': 11644, 'frame': 2, 'sequence': 'ATGCGCTACGTCTGCCGGACTGGGACGCGAAAATCGCTAACCTCTCCGGTGGTGAACGTCGTCGCGTAG', 'length': 69}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in orf_plus_nn[100:105]:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ-UL4MhkFEB",
        "outputId": "ea019b0e-6ecb-4e8b-9f31-e61fda3d4788"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'start': 12016, 'end': 12058, 'frame': 2, 'sequence': 'ATGACGTGGTTTACGACCCCATTTAGTAGTCAACCGCAGTGA', 'length': 42}\n",
            "{'start': 12074, 'end': 12092, 'frame': 3, 'sequence': 'ATGAAATTGGGCAGTTGA', 'length': 18}\n",
            "{'start': 12130, 'end': 12175, 'frame': 2, 'sequence': 'ATGATGACCGAATATATAGTGGAGACGTTTAGATGGGTAAAATAA', 'length': 45}\n",
            "{'start': 12162, 'end': 14079, 'frame': 1, 'sequence': 'ATGGGTAAAATAATTGGTATCGACCTGGGTACTACCAACTCTTGTGTAGCGATTATGGATGGCACCACTCCTCGCGTGCTGGAGAACGCCGAAGGCGATCGCACCACGCCTTCTATCATTGCCTATACCCAGGATGGTGAAACTCTAGTTGGTCAGCCGGCTAAACGTCAGGCAGTGACGAACCCGCAAAACACTCTGTTTGCGATTAAACGCCTGATTGGTCGCCGCTTCCAGGACGAAGAAGTACAGCGTGATGTTTCCATCATGCCGTTCAAAATTATTGCTGCTGATAACGGCGACGCATGGGTCGAAGTTAAAGGCCAGAAAATGGCACCGCCGCAGATTTCTGCTGAAGTGCTGAAAAAAATGAAGAAAACCGCTGAAGATTACCTGGGTGAACCGGTAACTGAAGCTGTTATCACCGTACCGGCATACTTTAACGATGCTCAGCGTCAGGCAACCAAAGACGCAGGCCGTATCGCTGGTCTGGAAGTAAAACGTATCATCAACGAACCGACCGCAGCTGCGCTGGCTTACGGTCTGGACAAAGGCACTGGCAACCGTACTATCGCGGTTTATGACCTGGGTGGTGGTACTTTCGATATTTCTATTATCGAAATCGACGAAGTTGACGGCGAAAAAACCTTCGAAGTTCTGGCAACCAACGGTGATACCCACCTGGGGGGTGAAGACTTCGACAGCCGTCTGATCAACTATCTGGTTGAAGAATTCAAGAAAGATCAGGGCATTGACCTGCGCAACGATCCGCTGGCAATGCAGCGCCTGAAAGAAGCGGCAGAAAAAGCGAAAATCGAACTGTCTTCCGCTCAGCAGACCGACGTTAACCTGCCATACATCACTGCAGACGCGACCGGTCCGAAACACATGAACATCAAAGTGACTCGTGCGAAACTGGAAAGCCTGGTTGAAGATCTGGTAAACCGTTCCATTGAGCCGCTGAAAGTTGCACTGCAGGACGCTGGCCTGTCCGTATCTGATATCGACGACGTTATCCTCGTTGGTGGTCAGACTCGTATGCCAATGGTTCAGAAGAAAGTTGCTGAGTTCTTTGGTAAAGAGCCGCGTAAAGACGTTAACCCGGACGAAGCTGTAGCAATCGGTGCTGCTGTTCAGGGTGGTGTTCTGACTGGTGACGTAAAAGACGTACTGCTGCTGGACGTTACCCCGCTGTCTCTGGGTATCGAAACCATGGGCGGTGTGATGACGACGCTGATCGCGAAAAACACCACTATCCCGACCAAGCACAGCCAGGTGTTCTCTACCGCTGAAGACAACCAGTCTGCGGTAACCATCCATGTGCTGCAGGGTGAACGTAAACGTGCGGCTGATAACAAATCTCTGGGTCAGTTCAACCTAGATGGTATCAACCCGGCACCGCGCGGCATGCCGCAGATCGAAGTTACCTTCGATATCGATGCTGACGGTATCCTGCACGTTTCCGCGAAAGATAAAAACAGCGGTAAAGAGCAGAAGATCACCATCAAGGCTTCTTCTGGTCTGAACGAAGATGAAATCCAGAAAATGGTACGCGACGCAGAAGCTAACGCCGAAGCTGACCGTAAGTTTGAAGAGCTGGTACAGACTCGCAACCAGGGCGACCATCTGCTGCACAGCACCCGTAAGCAGGTTGAAGAAGCAGGCGACAAACTGCCGGCTGACGACAAAACTGCTATCGAGTCTGCGCTGACTGCACTGGAAACTGCTCTGAAAGGTGAAGACAAAGCCGCTATCGAAGCGAAAATGCAGGAACTGGCACAGGTTTCCCAGAAACTGATGGAAATCGCCCAGCAGCAACATGCCCAGCAGCAGACTGCCGGTGCTGATGCTTCTGCAAACAACGCGAAAGATGACGATGTTGTCGACGCTGAATTTGAAGAAGTCAAAGACAAAAAATAA', 'length': 1917}\n",
            "{'start': 12220, 'end': 12310, 'frame': 2, 'sequence': 'ATGGCACCACTCCTCGCGTGCTGGAGAACGCCGAAGGCGATCGCACCACGCCTTCTATCATTGCCTATACCCAGGATGGTGAAACTCTAG', 'length': 90}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: а мы можем распрделение длин нарисовать для orf_plus_nn\n",
        "\n",
        "def plot_orf_length_distribution(orfs_list, title=\"Distribution of ORF Lengths\"):\n",
        "    \"\"\"\n",
        "    Строит гистограмму распределения длин ОРФ.\n",
        "\n",
        "    Args:\n",
        "        orfs_list (list): Список словарей, где каждый словарь представляет ОРФ\n",
        "                          и содержит ключ 'length'.\n",
        "        title (str): Заголовок для гистограммы.\n",
        "    \"\"\"\n",
        "    if not orfs_list:\n",
        "        print(f\"No ORFs found to plot distribution for: {title}\")\n",
        "        return\n",
        "\n",
        "    orf_lengths = [orf['length'] for orf in orfs_list]\n",
        "\n",
        "    # Находим самый короткий и самый длинный ОРФ для вывода\n",
        "    min_length = min(orf_lengths) if orf_lengths else 0\n",
        "    max_length = max(orf_lengths) if orf_lengths else 0\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Автоматически выбираем количество бинов, чтобы было достаточно деталей\n",
        "    plt.hist(orf_lengths, bins='auto', edgecolor='black')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"ORF Length (bp)\")\n",
        "    plt.ylabel(\"Number of ORFs\")\n",
        "    plt.grid(axis='y', alpha=0.75)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Total number of ORFs: {len(orf_lengths)}\")\n",
        "    print(f\"Shortest ORF length: {min_length} bp\")\n",
        "    print(f\"Longest ORF length:  {max_length} bp\")\n",
        "    if orf_lengths:\n",
        "      print(f\"Average ORF length:  {sum(orf_lengths) / len(orf_lengths):.2f} bp\")\n",
        "\n",
        "\n",
        "# Построение распределения для orf_plus_nn\n",
        "plot_orf_length_distribution(orf_plus_nn, title=\"Distribution of ORF Lengths on Plus Strand\")\n",
        "\n",
        "# Если вам нужно построить распределение для orfs_minus_nn тоже\n",
        "orf_minus_nn = find_orfs_bacterial(reverse_complement(sequence))\n",
        "plot_orf_length_distribution(orf_minus_nn, title=\"Distribution of ORF Lengths on Minus Strand (Reverse Complement)\")\n"
      ],
      "metadata": {
        "id": "BmKdtnnwkYWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio"
      ],
      "metadata": {
        "id": "819ncx2Bm-Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import SeqIO\n",
        "import csv\n",
        "\n",
        "def extract_orfs_from_gff(gff_file, fasta_file):\n",
        "    # Считываем последовательности\n",
        "    seq_dict = SeqIO.to_dict(SeqIO.parse(fasta_file, \"fasta\"))\n",
        "    orfs = []\n",
        "\n",
        "    with open(gff_file, 'r') as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            if row[0].startswith('#') or row[2] != 'gene':\n",
        "                continue\n",
        "            chrom = row[0]\n",
        "            start = int(row[3])-1\n",
        "            end = int(row[4])\n",
        "            strand = row[6]\n",
        "\n",
        "            sequence = seq_dict[chrom].seq[start:end]  # GFF is 0-based\n",
        "            if strand == '-':\n",
        "                continue\n",
        "                sequence = sequence.reverse_complement()\n",
        "\n",
        "            frame = (start - 1) % 3\n",
        "            orfs.append({\n",
        "                'start': start,\n",
        "                'end': end,\n",
        "                'frame': frame,\n",
        "                'sequence': str(sequence),\n",
        "                'length': end - start + 1\n",
        "            })\n",
        "\n",
        "    return orfs\n",
        "\n",
        "# Пример использования\n",
        "gff_file = \"GCF_000005845.2_ASM584v2_genomic.gff\"\n",
        "fasta_file = \"GCF_000005845.2_ASM584v2_genomic.fna\"\n",
        "reference_orfs = extract_orfs_from_gff(gff_file, fasta_file)\n",
        "\n",
        "for orf in reference_orfs[:1]:\n",
        "    print(orf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_RNexlZlTAz",
        "outputId": "377ae334-8ef7-4b75-a34b-00acce223ab8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'start': 189, 'end': 255, 'frame': 2, 'sequence': 'ATGAAACGCATTAGCACCACCATTACCACCACCATCACCATTACCACAGGTAACGGTGCGGGCTGA', 'length': 67}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(reference_orfs), len(orf_plus_nn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uO-P5i4m7wk",
        "outputId": "70ed8979-d55d-4c62-a4a4-48b8ab2e34c3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2209, 40954)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "def orfs_to_intervals(orfs):\n",
        "    return [(orf['start'], orf['end']) for orf in orfs]\n",
        "\n",
        "def match_orfs(predicted, truth, tolerance=0):\n",
        "    pred_intervals = orfs_to_intervals(predicted)\n",
        "    true_intervals = orfs_to_intervals(truth)\n",
        "    matched_true = set()\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "\n",
        "    for p in pred_intervals:\n",
        "        found = False\n",
        "        for idx, t in enumerate(true_intervals):\n",
        "            if idx in matched_true:\n",
        "                continue\n",
        "            # Допускаем погрешность (например, +/- 3 нуклеотида)\n",
        "            if abs(p[0] - t[0]) <= tolerance and abs(p[1] - t[1]) <= tolerance:\n",
        "                tp += 1\n",
        "                matched_true.add(idx)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            fp += 1\n",
        "\n",
        "    fn = len(true_intervals) - len(matched_true)\n",
        "    return tp, fp, fn\n",
        "\n",
        "def compute_metrics(tp, fp, fn):\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    accuracy  = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0.0\n",
        "    return {\n",
        "        'TP': tp,\n",
        "        'FP': fp,\n",
        "        'FN': fn,\n",
        "        'Accuracy': round(accuracy, 4),\n",
        "        'Precision': round(precision, 4),\n",
        "        'Recall': round(recall, 4),\n",
        "        'F1 Score': round(f1, 4),\n",
        "    }\n",
        "\n",
        "def print_metrics_table(metrics):\n",
        "    df = pd.DataFrame([metrics])\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "# 📦 Пример использования\n",
        "# orfs1 = [...]  # предсказания\n",
        "# orfs2 = [...]  # истинные ORF'ы\n",
        "\n",
        "# Пример:\n",
        "# orfs1 = [{'start': 100, 'end': 200}, {'start': 300, 'end': 400}]\n",
        "# orfs2 = [{'start': 100, 'end': 200}, {'start': 500, 'end': 600}]\n",
        "\n",
        "def evaluate_datasets(orf_plus_nn, reference_orfs, tolerance=0):\n",
        "    tp, fp, fn = match_orfs(orf_plus_nn, reference_orfs, tolerance)\n",
        "    metrics = compute_metrics(tp, fp, fn)\n",
        "    print_metrics_table(metrics)"
      ],
      "metadata": {
        "id": "dqDcQB2gn4xK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_datasets(orf_plus_nn, reference_orfs, tolerance=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV-gt8UBoghm",
        "outputId": "d3f22086-9b9b-4878-ebac-cb39b70eb4f0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  TP    FP  FN  Accuracy  Precision  Recall  F1 Score\n",
            "1569 39385 640    0.0377     0.0383  0.7103    0.0727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_datasets(orf_plus_nn, reference_orfs, tolerance=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaEr3WExonVk",
        "outputId": "139b0527-7e1d-4af1-f4d5-d9d53509d41b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  TP    FP  FN  Accuracy  Precision  Recall  F1 Score\n",
            "1588 39366 621    0.0382     0.0388  0.7189    0.0736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_datasets(orf_plus_nn, reference_orfs, tolerance=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEaJV6liowoL",
        "outputId": "b03bbe9c-3b7b-49ac-f079-dfb619b85824"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  TP    FP  FN  Accuracy  Precision  Recall  F1 Score\n",
            "1569 39385 640    0.0377     0.0383  0.7103    0.0727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(orf_plus_nn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZdfz-WbqS3Q",
        "outputId": "c77cf779-ba80-4e34-a62e-352d999b7a62"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40954"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in orf_plus_nn[:5]:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkw8DCucqw7s",
        "outputId": "36541f06-29b4-4dbb-c165-4e93041d11ab"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'start': 29, 'end': 98, 'frame': 3, 'sequence': 'ATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGCTTCTGAACTGGTTACCTGCCGTGAGTAA', 'length': 69}\n",
            "{'start': 189, 'end': 255, 'frame': 1, 'sequence': 'ATGAAACGCATTAGCACCACCATTACCACCACCATCACCATTACCACAGGTAACGGTGCGGGCTGA', 'length': 66}\n",
            "{'start': 336, 'end': 2799, 'frame': 1, 'sequence': 'ATGCGAGTGTTGAAGTTCGGCGGTACATCAGTGGCAAATGCAGAACGTTTTCTGCGTGTTGCCGATATTCTGGAAAGCAATGCCAGGCAGGGGCAGGTGGCCACCGTCCTCTCTGCCCCCGCCAAAATCACCAACCACCTGGTGGCGATGATTGAAAAAACCATTAGCGGCCAGGATGCTTTACCCAATATCAGCGATGCCGAACGTATTTTTGCCGAACTTTTGACGGGACTCGCCGCCGCCCAGCCGGGGTTCCCGCTGGCGCAATTGAAAACTTTCGTCGATCAGGAATTTGCCCAAATAAAACATGTCCTGCATGGCATTAGTTTGTTGGGGCAGTGCCCGGATAGCATCAACGCTGCGCTGATTTGCCGTGGCGAGAAAATGTCGATCGCCATTATGGCCGGCGTATTAGAAGCGCGCGGTCACAACGTTACTGTTATCGATCCGGTCGAAAAACTGCTGGCAGTGGGGCATTACCTCGAATCTACCGTCGATATTGCTGAGTCCACCCGCCGTATTGCGGCAAGCCGCATTCCGGCTGATCACATGGTGCTGATGGCAGGTTTCACCGCCGGTAATGAAAAAGGCGAACTGGTGGTGCTTGGACGCAACGGTTCCGACTACTCTGCTGCGGTGCTGGCTGCCTGTTTACGCGCCGATTGTTGCGAGATTTGGACGGACGTTGACGGGGTCTATACCTGCGACCCGCGTCAGGTGCCCGATGCGAGGTTGTTGAAGTCGATGTCCTACCAGGAAGCGATGGAGCTTTCCTACTTCGGCGCTAAAGTTCTTCACCCCCGCACCATTACCCCCATCGCCCAGTTCCAGATCCCTTGCCTGATTAAAAATACCGGAAATCCTCAAGCACCAGGTACGCTCATTGGTGCCAGCCGTGATGAAGACGAATTACCGGTCAAGGGCATTTCCAATCTGAATAACATGGCAATGTTCAGCGTTTCTGGTCCGGGGATGAAAGGGATGGTCGGCATGGCGGCGCGCGTCTTTGCAGCGATGTCACGCGCCCGTATTTCCGTGGTGCTGATTACGCAATCATCTTCCGAATACAGCATCAGTTTCTGCGTTCCACAAAGCGACTGTGTGCGAGCTGAACGGGCAATGCAGGAAGAGTTCTACCTGGAACTGAAAGAAGGCTTACTGGAGCCGCTGGCAGTGACGGAACGGCTGGCCATTATCTCGGTGGTAGGTGATGGTATGCGCACCTTGCGTGGGATCTCGGCGAAATTCTTTGCCGCACTGGCCCGCGCCAATATCAACATTGTCGCCATTGCTCAGGGATCTTCTGAACGCTCAATCTCTGTCGTGGTAAATAACGATGATGCGACCACTGGCGTGCGCGTTACTCATCAGATGCTGTTCAATACCGATCAGGTTATCGAAGTGTTTGTGATTGGCGTCGGTGGCGTTGGCGGTGCGCTGCTGGAGCAACTGAAGCGTCAGCAAAGCTGGCTGAAGAATAAACATATCGACTTACGTGTCTGCGGTGTTGCCAACTCGAAGGCTCTGCTCACCAATGTACATGGCCTTAATCTGGAAAACTGGCAGGAAGAACTGGCGCAAGCCAAAGAGCCGTTTAATCTCGGGCGCTTAATTCGCCTCGTGAAAGAATATCATCTGCTGAACCCGGTCATTGTTGACTGCACTTCCAGCCAGGCAGTGGCGGATCAATATGCCGACTTCCTGCGCGAAGGTTTCCACGTTGTCACGCCGAACAAAAAGGCCAACACCTCGTCGATGGATTACTACCATCAGTTGCGTTATGCGGCGGAAAAATCGCGGCGTAAATTCCTCTATGACACCAACGTTGGGGCTGGATTACCGGTTATTGAGAACCTGCAAAATCTGCTCAATGCAGGTGATGAATTGATGAAGTTCTCCGGCATTCTTTCTGGTTCGCTTTCTTATATCTTCGGCAAGTTAGACGAAGGCATGAGTTTCTCCGAGGCGACCACGCTGGCGCGGGAAATGGGTTATACCGAACCGGACCCGCGAGATGATCTTTCTGGTATGGATGTGGCGCGTAAACTATTGATTCTCGCTCGTGAAACGGGACGTGAACTGGAGCTGGCGGATATTGAAATTGAACCTGTGCTGCCCGCAGAGTTTAACGCCGAGGGTGATGTTGCCGCTTTTATGGCGAATCTGTCACAACTCGACGATCTCTTTGCCGCGCGCGTGGCGAAGGCCCGTGATGAAGGAAAAGTTTTGCGCTATGTTGGCAATATTGATGAAGATGGCGTCTGCCGCGTGAAGATTGCCGAAGTGGATGGTAATGATCCGCTGTTCAAAGTGAAAAATGGCGAAAACGCCCTGGCCTTCTATAGCCACTATTATCAGCCGCTGCCGTTGGTACTGCGCGGATATGGTGCGGGCAATGACGTTACAGCTGCCGGTGTCTTTGCTGATCTGCTACGTACCCTCTCATGGAAGTTAGGAGTCTGA', 'length': 2463}\n",
            "{'start': 373, 'end': 487, 'frame': 2, 'sequence': 'ATGCAGAACGTTTTCTGCGTGTTGCCGATATTCTGGAAAGCAATGCCAGGCAGGGGCAGGTGGCCACCGTCCTCTCTGCCCCCGCCAAAATCACCAACCACCTGGTGGCGATGA', 'length': 114}\n",
            "{'start': 511, 'end': 562, 'frame': 2, 'sequence': 'ATGCTTTACCCAATATCAGCGATGCCGAACGTATTTTTGCCGAACTTTTGA', 'length': 51}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orf_plus_filtered_nn = [x for x in orf_plus_nn if x[\"length\"] > 5 * 3]"
      ],
      "metadata": {
        "id": "eEc2imC_pE3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(orf_plus_filtered_nn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5t-Mhktqs4F",
        "outputId": "aede363a-3540-4441-d14d-bf00d3966019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34519"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_datasets(orf_plus_filtered_nn, reference_orfs, tolerance=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBQLZTe-p6cd",
        "outputId": "5b171e09-65fe-4a52-f913-d0a334229770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  TP    FP  FN  Accuracy  Precision  Recall  F1 Score\n",
            "1569 32950 640    0.0446     0.0455  0.7103    0.0854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   TP    FP  FN  Accuracy  Precision  Recall  F1 Score\n",
        "# 1569 39385 640    0.0377     0.0383  0.7103    0.0727\n",
        "# 1569 32950 640    0.0446     0.0455  0.7103    0.0854\n",
        "# 1557 17281 652    0.0799     0.0827  0.7048     0.148\n"
      ],
      "metadata": {
        "id": "1921PDOop8H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_orfs(predicted, truth, tolerance=0):\n",
        "    pred_intervals = [(p['start'], p['end']) for p in predicted]\n",
        "    true_intervals = [(t['start'], t['end']) for t in truth]\n",
        "\n",
        "    matched_pred = set()\n",
        "    matched_true = set()\n",
        "\n",
        "    tagged_pred = []\n",
        "    tagged_true = []\n",
        "\n",
        "    for i, p in enumerate(pred_intervals):\n",
        "        matched = False\n",
        "        for j, t in enumerate(true_intervals):\n",
        "            if j in matched_true:\n",
        "                continue\n",
        "            if abs(p[0] - t[0]) <= tolerance and abs(p[1] - t[1]) <= tolerance:\n",
        "                matched_pred.add(i)\n",
        "                matched_true.add(j)\n",
        "                matched = True\n",
        "                break\n",
        "        tag = 'TP' if matched else 'FP'\n",
        "        tagged_pred.append({**predicted[i], 'type': tag})\n",
        "\n",
        "    for j, t in enumerate(true_intervals):\n",
        "        tag = 'TP' if j in matched_true else 'FN'\n",
        "        tagged_true.append({**truth[j], 'type': tag})\n",
        "\n",
        "    return tagged_pred, tagged_true"
      ],
      "metadata": {
        "id": "TRTBqk8grnwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ho3oam4nsEtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_tagged, true_tagged = tag_orfs(orf_plus_filtered_nn, reference_orfs, tolerance=0)\n",
        "\n",
        "# Пример вывода\n",
        "import pandas as pd\n",
        "\n",
        "df_pred = pd.DataFrame(predicted_tagged)\n",
        "df_true = pd.DataFrame(true_tagged)\n",
        "\n",
        "print(\"Предсказания:\")\n",
        "print(df_pred[['start', 'end', 'type']].to_string(index=False))\n",
        "\n",
        "print(\"\\nИстина:\")\n",
        "print(df_true[['start', 'end', 'type']].to_string(index=False))"
      ],
      "metadata": {
        "id": "ByW6WY2QrqHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_errors_only(tagged_predicted, tagged_truth):\n",
        "    print(\"\\nОшибки в предсказаниях (FP):\")\n",
        "    for orf in tagged_predicted:\n",
        "        if orf['type'] != 'TP':\n",
        "            print(f\"END: {orf['end']} | TYPE: {orf['type']} | SEQUENCE: {orf['sequence']}\")\n",
        "\n",
        "    print(\"\\nПропущенные гены (FN):\")\n",
        "    for orf in tagged_truth:\n",
        "        if orf['type'] != 'TP':\n",
        "            print(f\"END: {orf['end']} | TYPE: {orf['type']} | SEQUENCE: {orf['sequence']}\")"
      ],
      "metadata": {
        "id": "LOjKS_PbrrlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_tagged, truth_tagged = tag_orfs(orf_plus_filtered_nn, reference_orfs, tolerance=0)\n",
        "print_errors_only(predicted_tagged, truth_tagged)"
      ],
      "metadata": {
        "id": "TuxEhdDXsGs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_orfs_with_start_stop_analysis(predicted, truth):\n",
        "    matched_true = set()\n",
        "    tagged_pred = []\n",
        "    tagged_true = []\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        match_type = 'FP'\n",
        "        for j, t in enumerate(truth):\n",
        "            if j in matched_true:\n",
        "                continue\n",
        "\n",
        "            match_start = p['start'] == t['start']\n",
        "            match_end = p['end'] == t['end']\n",
        "\n",
        "            if match_start and match_end:\n",
        "                matched_true.add(j)\n",
        "                match_type = 'TP-full'\n",
        "                break\n",
        "            elif match_start:\n",
        "                matched_true.add(j)\n",
        "                match_type = 'TP-start'\n",
        "                break\n",
        "            elif match_end:\n",
        "                matched_true.add(j)\n",
        "                match_type = 'TP-end'\n",
        "                break\n",
        "\n",
        "        tagged_pred.append({**p, 'type': match_type})\n",
        "\n",
        "    for j, t in enumerate(truth):\n",
        "        tag = 'FN' if j not in matched_true else 'TP'\n",
        "        tagged_true.append({**t, 'type': tag})\n",
        "\n",
        "    return tagged_pred, tagged_true"
      ],
      "metadata": {
        "id": "Sxv0aNFXsHrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def summarize_start_stop_errors(tagged_pred):\n",
        "    counter = Counter([orf['type'] for orf in tagged_pred])\n",
        "    print(\"\\nАнализ ошибок по совпадениям:\")\n",
        "    for key in ['TP-full', 'TP-start', 'TP-end', 'FP']:\n",
        "        print(f\"{key}: {counter.get(key, 0)}\")"
      ],
      "metadata": {
        "id": "RvjT8wxTuiyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_tagged, true_tagged = tag_orfs_with_start_stop_analysis(orf_plus_filtered_nn, reference_orfs)\n",
        "summarize_start_stop_errors(predicted_tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5zj5lINulCi",
        "outputId": "2bdf3680-bd8e-47b7-ecdc-87c2c611e024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Анализ ошибок по совпадениям:\n",
            "TP-full: 1569\n",
            "TP-start: 1\n",
            "TP-end: 505\n",
            "FP: 32444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import statistics\n",
        "import re # Может понадобиться для более сложных операций с мотивами, пока не используется активно\n",
        "\n",
        "# --- Вспомогательные функции ---\n",
        "\n",
        "def read_fasta(filepath: str) -> dict:\n",
        "    \"\"\"\n",
        "    Читает FASTA файл и возвращает словарь {заголовок: последовательность}.\n",
        "    Заголовком считается первая часть до пробела. Последовательности приводятся к верхнему регистру.\n",
        "    \"\"\"\n",
        "    sequences = {}\n",
        "    current_seq_header = None\n",
        "    current_seq_list = []\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line: # Пропускаем пустые строки\n",
        "                    continue\n",
        "                if line.startswith(\">\"):\n",
        "                    if current_seq_header: # Сохраняем предыдущую последовательность\n",
        "                        sequences[current_seq_header] = \"\".join(current_seq_list)\n",
        "                    current_seq_header = line[1:].split()[0] # Берем ID до первого пробела\n",
        "                    current_seq_list = []\n",
        "                else:\n",
        "                    if current_seq_header: # Убедимся, что заголовок уже был\n",
        "                        current_seq_list.append(line.upper()) # Приводим к верхнему регистру\n",
        "        if current_seq_header: # Сохраняем последнюю последовательность в файле\n",
        "            sequences[current_seq_header] = \"\".join(current_seq_list)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Ошибка: Файл {filepath} не найден.\")\n",
        "        raise # Передаем исключение дальше\n",
        "    if not sequences:\n",
        "        print(f\"Предупреждение: Файл {filepath} пуст или не содержит последовательностей в формате FASTA.\")\n",
        "    return sequences\n",
        "\n",
        "def reverse_complement(dna_seq: str) -> str:\n",
        "    \"\"\"Вычисляет обратную комплементарную последовательность ДНК.\"\"\"\n",
        "    if not isinstance(dna_seq, str):\n",
        "        raise TypeError(\"Input sequence must be a string.\")\n",
        "    # Убедимся, что последовательность в верхнем регистре перед трансляцией\n",
        "    complement_map = str.maketrans(\"ATGC\", \"TACG\")\n",
        "    return dna_seq.upper().translate(complement_map)[::-1]\n",
        "\n",
        "# --- Функция обучения параметров ---\n",
        "\n",
        "def train_orf_parameters(\n",
        "    gff_filepath: str,\n",
        "    reference_fasta_filepath: str,\n",
        "    candidate_sd_motifs: list,\n",
        "    sd_upstream_window_rel_to_atg: tuple = (-15, -4) # (-15 до -5 включительно)\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Вычисляет параметры для фильтрации ОРФов на основе референсного GFF и генома.\n",
        "\n",
        "    Args:\n",
        "        gff_filepath: Путь к GFF3 файлу.\n",
        "        reference_fasta_filepath: Путь к FASTA файлу референсного генома.\n",
        "        candidate_sd_motifs: Список предполагаемых SD-мотивов для проверки (например, [\"AGGAGG\", \"AAGGAG\"]).\n",
        "        sd_upstream_window_rel_to_atg: Кортеж (старт, конец_не_включая) определяющий окно\n",
        "                                      перед 'A' кодона ATG (A=0) для поиска SD-мотивов.\n",
        "                                      Например, (-15, -4) означает с -15 п.н. (включительно) до -4 п.н. (не включая).\n",
        "    Returns:\n",
        "        Словарь с натренированными параметрами или пустой словарь при ошибке.\n",
        "    \"\"\"\n",
        "    print(f\"Запуск обучения параметров ОРФ из {gff_filepath} и {reference_fasta_filepath}...\")\n",
        "\n",
        "    try:\n",
        "        reference_genome_dict = read_fasta(reference_fasta_filepath)\n",
        "        if not reference_genome_dict:\n",
        "            print(f\"Ошибка: Не удалось прочитать последовательности из {reference_fasta_filepath}.\")\n",
        "            return {}\n",
        "    except FileNotFoundError:\n",
        "        # read_fasta уже печатает сообщение, можно просто вернуть пустой словарь\n",
        "        return {}\n",
        "\n",
        "    all_ref_cds_sequences_oriented = []\n",
        "\n",
        "    print(\"Парсинг GFF для извлечения CDS признаков...\")\n",
        "    cds_found_in_gff_total = 0\n",
        "    cds_processed_valid = 0\n",
        "    skipped_stats = collections.defaultdict(int)\n",
        "\n",
        "    try:\n",
        "        with open(gff_filepath, 'r') as f_gff:\n",
        "            for line_num, line in enumerate(f_gff, 1):\n",
        "                if line.startswith(\"#\") or not line.strip():\n",
        "                    continue\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) < 9:\n",
        "                    skipped_stats['invalid_gff_line_format'] +=1\n",
        "                    continue\n",
        "\n",
        "                feature_type = parts[2]\n",
        "                if feature_type.upper() == 'CDS':\n",
        "                    cds_found_in_gff_total += 1\n",
        "                    seq_id = parts[0]\n",
        "                    try:\n",
        "                        gff_start_1based = int(parts[3])\n",
        "                        gff_end_1based = int(parts[4])\n",
        "                    except ValueError:\n",
        "                        skipped_stats['invalid_cds_coordinates'] +=1\n",
        "                        continue\n",
        "\n",
        "                    strand = parts[6]\n",
        "                    if strand not in ['+', '-']:\n",
        "                        skipped_stats['invalid_cds_strand'] +=1\n",
        "                        continue\n",
        "\n",
        "                    if seq_id not in reference_genome_dict:\n",
        "                        skipped_stats['seq_id_not_in_fasta'] +=1\n",
        "                        continue\n",
        "\n",
        "                    contig_sequence = reference_genome_dict[seq_id]\n",
        "                    contig_len = len(contig_sequence)\n",
        "\n",
        "                    cds_genomic_start_0based = gff_start_1based - 1\n",
        "                    cds_genomic_end_0based_exclusive = gff_end_1based\n",
        "\n",
        "                    if not (0 <= cds_genomic_start_0based < cds_genomic_end_0based_exclusive <= contig_len):\n",
        "                        skipped_stats['cds_coords_out_of_bounds'] +=1\n",
        "                        continue\n",
        "\n",
        "                    cds_fragment_on_contig = contig_sequence[cds_genomic_start_0based : cds_genomic_end_0based_exclusive]\n",
        "\n",
        "                    oriented_cds_sequence = cds_fragment_on_contig\n",
        "                    if strand == '-':\n",
        "                        oriented_cds_sequence = reverse_complement(cds_fragment_on_contig)\n",
        "\n",
        "                    if not oriented_cds_sequence.startswith(\"ATG\"):\n",
        "                        skipped_stats['cds_not_starting_ATG'] +=1\n",
        "                        continue\n",
        "                    if len(oriented_cds_sequence) % 3 != 0:\n",
        "                        skipped_stats['cds_len_not_multiple_of_3'] +=1\n",
        "                        continue\n",
        "                    stop_codon = oriented_cds_sequence[-3:]\n",
        "                    if stop_codon not in [\"TAA\", \"TAG\", \"TGA\"]:\n",
        "                        skipped_stats['cds_no_standard_stop'] +=1\n",
        "                        continue\n",
        "\n",
        "                    all_ref_cds_sequences_oriented.append(oriented_cds_sequence)\n",
        "                    cds_processed_valid +=1\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Ошибка: GFF файл {gff_filepath} не найден.\")\n",
        "        return {} # Возвращаем пустой словарь\n",
        "    except Exception as e:\n",
        "        print(f\"Произошла ошибка при чтении GFF файла {gff_filepath}: {e}\")\n",
        "        return {}\n",
        "\n",
        "\n",
        "    print(f\"  Всего найдено CDS записей в GFF: {cds_found_in_gff_total}\")\n",
        "    for reason, count in skipped_stats.items():\n",
        "        print(f\"  Пропущено CDS по причине '{reason}': {count}\")\n",
        "    print(f\"  Используется {cds_processed_valid} валидных референсных CDS для обучения.\")\n",
        "\n",
        "    if not all_ref_cds_sequences_oriented:\n",
        "        print(\"Ошибка: Не найдено валидных CDS в GFF для обучения. Проверьте GFF, FASTA и их соответствие.\")\n",
        "        return {}\n",
        "\n",
        "    all_codons_for_stats = []\n",
        "    gc1_vals, gc2_vals, gc3_vals = [], [], []\n",
        "\n",
        "    for cds_seq in all_ref_cds_sequences_oriented:\n",
        "        # Анализируем кодоны, исключая стоп-кодон, для таблиц частот и GC-статистики\n",
        "        # Старт-кодон (ATG) включается в анализ кодонов\n",
        "        for i in range(0, len(cds_seq) - 3, 3): # -3 чтобы исключить стоп-кодон\n",
        "            codon = cds_seq[i:i+3]\n",
        "            all_codons_for_stats.append(codon)\n",
        "\n",
        "            gc1_vals.append(1 if codon[0] in 'GC' else 0)\n",
        "            gc2_vals.append(1 if codon[1] in 'GC' else 0)\n",
        "            gc3_vals.append(1 if codon[2] in 'GC' else 0)\n",
        "\n",
        "    if not all_codons_for_stats: # Должно быть избыточным, если all_ref_cds_sequences_oriented не пуст\n",
        "        print(\"Ошибка: Не удалось извлечь кодоны из референсных CDS. Обучение невозможно.\")\n",
        "        return {}\n",
        "\n",
        "    codon_counts = collections.Counter(all_codons_for_stats)\n",
        "    total_valid_codons = sum(codon_counts.values())\n",
        "    codon_frequencies_ref = {c: count / total_valid_codons for c, count in codon_counts.items()}\n",
        "\n",
        "    mean_gc3_ref = statistics.mean(gc3_vals) if gc3_vals else 0.5 # 0.5 как нейтральное значение\n",
        "    std_gc3_ref = statistics.stdev(gc3_vals) if len(gc3_vals) > 1 else 0.0 # Если 0 или 1 значение, stdev не определен или 0\n",
        "\n",
        "    trained_params = {\n",
        "        \"codon_frequencies_reference\": codon_frequencies_ref,\n",
        "        \"gc3_mean_reference\": mean_gc3_ref,\n",
        "        \"gc3_std_reference\": std_gc3_ref,\n",
        "        \"sd_motifs_to_check\": candidate_sd_motifs,\n",
        "        \"sd_search_window_rel_to_atg\": sd_upstream_window_rel_to_atg\n",
        "    }\n",
        "    print(\"Обучение параметров завершено.\")\n",
        "    print(f\"  Референсная частота кодонов: {len(trained_params['codon_frequencies_reference'])} типов кодонов\")\n",
        "    print(f\"  GC3 среднее (реф): {trained_params['gc3_mean_reference']:.3f}\")\n",
        "    print(f\"  GC3 станд.откл. (реф): {trained_params['gc3_std_reference']:.3f}\")\n",
        "    print(f\"  SD мотивы для проверки: {trained_params['sd_motifs_to_check']}\")\n",
        "    print(f\"  Окно поиска SD (относительно A в ATG=0): {trained_params['sd_search_window_rel_to_atg']}\")\n",
        "    return trained_params\n",
        "\n",
        "# --- Исходная функция поиска ОРФов ---\n",
        "def find_initial_orfs(dna_sequence: str, min_aa_length: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    Находит потенциальные ОРФы, начинающиеся с ATG и заканчивающиеся стоп-кодоном.\n",
        "    ОРФы должны иметь минимальную длину в аминокислотах (включая стартовый метионин, не считая стоп).\n",
        "    \"\"\"\n",
        "    if not dna_sequence or not isinstance(dna_sequence, str):\n",
        "        return []\n",
        "    dna_sequence = dna_sequence.upper()\n",
        "    n = len(dna_sequence)\n",
        "    start_codon = \"ATG\"\n",
        "    stop_codons = [\"TAA\", \"TAG\", \"TGA\"]\n",
        "    # Минимальная длина ОРФа в нуклеотидах: (min_aa_length + 1 стоп-кодон) * 3\n",
        "    # или (min_aa_length для белка, значит min_aa_length кодонов + стартовый кодон)\n",
        "    # Если min_aa_length = 5, то это Met + 4 AA + Stop = 6 кодонов = 18 п.н.\n",
        "    min_orf_len_nt = (min_aa_length + 1) * 3 # +1 для стартового кодона, если min_aa_length это число кодируемых AA после Met.\n",
        "                                            # Если min_aa_length включает Met, то (min_aa_length) * 3 для AA + 3 для Stop.\n",
        "                                            # Давайте считать min_aa_length как число кодируемых аминокислот ПОСЛЕ старта ИЛИ ВКЛЮЧАЯ старт.\n",
        "                                            # \"короче пяти аминокислот\" -> белок < 5 АА. Значит, белок >= 5АА.\n",
        "                                            # Met + 4AA + Stop -> 5 AA в белке. 6 кодонов. 18 п.н.\n",
        "    min_orf_len_nt = min_aa_length * 3 + 3 # Met + (N-1) AA + Stop. So N codons for protein. N*3 + 3 for Stop.\n",
        "                                           # Если min_aa_length = 5 (5 аминокислот в белке), то 5*3 (для Met+4AA) + 3 (для Stop) = 18 п.н.\n",
        "\n",
        "    found_orfs_details = []\n",
        "    # Словарь для хранения уже идентифицированных регионов ОРФ для каждой рамки.\n",
        "    # Ключ: рамка (0, 1, 2), Значение: список кортежей (start_abs_index, end_abs_index)\n",
        "    identified_orf_regions_by_frame = collections.defaultdict(list)\n",
        "\n",
        "    for frame_offset in range(3):\n",
        "        potential_atg_indices = []\n",
        "        for i in range(frame_offset, n - 2, 3): # n-2 чтобы codon был полным\n",
        "            if dna_sequence[i : i + 3] == start_codon:\n",
        "                potential_atg_indices.append(i)\n",
        "\n",
        "        for atg_start_idx in potential_atg_indices:\n",
        "            is_nested = False\n",
        "            for orf_s, orf_e in identified_orf_regions_by_frame[frame_offset]:\n",
        "                if orf_s <= atg_start_idx < orf_e: # Вложенный, если начало ATG внутри существующего ОРФа\n",
        "                    is_nested = True\n",
        "                    break\n",
        "            if is_nested:\n",
        "                continue\n",
        "\n",
        "            # Ищем первый стоп-кодон, начиная со следующего кодона после ATG\n",
        "            for j in range(atg_start_idx + 3, n - 2, 3):\n",
        "                codon = dna_sequence[j : j + 3]\n",
        "                if codon in stop_codons:\n",
        "                    orf_end_idx = j + 3 # Конечный индекс для среза (не включая)\n",
        "                    orf_seq = dna_sequence[atg_start_idx:orf_end_idx]\n",
        "                    orf_len = len(orf_seq)\n",
        "\n",
        "                    if orf_len >= min_orf_len_nt:\n",
        "                        orf_info = {\n",
        "                            \"start_0based\": atg_start_idx,\n",
        "                            \"end_0based_exclusive\": orf_end_idx,\n",
        "                            \"frame\": frame_offset + 1, # 1-based frame для пользователя\n",
        "                            \"sequence\": orf_seq, # Включает старт- и стоп-кодоны\n",
        "                            \"length\": orf_len\n",
        "                        }\n",
        "                        found_orfs_details.append(orf_info)\n",
        "                        # Запоминаем регион этого ОРФа для текущей рамки\n",
        "                        identified_orf_regions_by_frame[frame_offset].append((atg_start_idx, orf_end_idx))\n",
        "                    break # ОРФ заканчивается первым стоп-кодоном\n",
        "\n",
        "    # Опционально: отсортировать ОРФы, например, по стартовой позиции\n",
        "    found_orfs_details.sort(key=lambda x: (x['start_0based'], x['frame']))\n",
        "    return found_orfs_details\n",
        "\n",
        "# --- Расширенная функция поиска ОРФов с фильтрацией ---\n",
        "def find_orfs_enhanced(\n",
        "    dna_to_analyze: str,\n",
        "    trained_parameters: dict,\n",
        "    min_aa_length: int = 5,\n",
        "    gc3_std_dev_multiplier: float = 2.0 # Количество стандартных отклонений для GC3 порога\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Находит ОРФы и фильтрует их, используя натренированные параметры (SD мотивы, GC3 смещение).\n",
        "    ОРФы должны пройти ОБА фильтра.\n",
        "    \"\"\"\n",
        "    if not trained_parameters:\n",
        "        print(\"Ошибка: Параметры для фильтрации не натренированы или пусты. Запустите train_orf_parameters.\")\n",
        "        return []\n",
        "    if not dna_to_analyze or not isinstance(dna_to_analyze, str):\n",
        "        print(\"Ошибка: Предоставлена некорректная или пустая последовательность ДНК для анализа.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Запуск расширенного поиска ОРФов на последовательности длиной {len(dna_to_analyze)}...\")\n",
        "    dna_to_analyze = dna_to_analyze.upper()\n",
        "\n",
        "    initial_orfs = find_initial_orfs(dna_to_analyze, min_aa_length)\n",
        "    print(f\"  Найдено {len(initial_orfs)} первоначальных ОРФов (длина >= {min_aa_length} а.к.). Применение фильтров...\")\n",
        "\n",
        "    filtered_orfs = []\n",
        "\n",
        "    # Извлечение параметров для удобства\n",
        "    sd_motifs_to_check = trained_parameters.get(\"sd_motifs_to_check\", [])\n",
        "    sd_search_window_rel_to_atg = trained_parameters.get(\"sd_search_window_rel_to_atg\", (-15,-4))\n",
        "    gc3_mean_ref = trained_parameters.get(\"gc3_mean_reference\", 0.5) # Нейтральное значение по умолчанию\n",
        "    gc3_std_ref = trained_parameters.get(\"gc3_std_reference\", 0.0)\n",
        "\n",
        "    sd_win_start_rel, sd_win_end_rel_excl = sd_search_window_rel_to_atg\n",
        "\n",
        "    orfs_passed_sd_count = 0\n",
        "    orfs_passed_all_count = 0\n",
        "\n",
        "    for orf in initial_orfs:\n",
        "        # 1. Фильтр по SD-последовательности\n",
        "        has_sd_signal = False\n",
        "        if sd_motifs_to_check: # Применять фильтр только если есть мотивы для проверки\n",
        "            orf_genomic_start_0based = orf[\"start_0based\"]\n",
        "\n",
        "            # Абсолютные координаты окна для поиска SD в dna_to_analyze\n",
        "            upstream_search_abs_start = orf_genomic_start_0based + sd_win_start_rel\n",
        "            upstream_search_abs_end_excl = orf_genomic_start_0based + sd_win_end_rel_excl\n",
        "\n",
        "            # Окно должно быть перед ATG (т.е. upstream_search_abs_end_excl <= orf_genomic_start_0based)\n",
        "            # и в пределах последовательности (upstream_search_abs_start >= 0)\n",
        "            if upstream_search_abs_start >= 0 and \\\n",
        "               upstream_search_abs_end_excl <= orf_genomic_start_0based and \\\n",
        "               upstream_search_abs_end_excl > upstream_search_abs_start: # Окно должно иметь положительную длину\n",
        "\n",
        "                upstream_sequence_for_sd = dna_to_analyze[upstream_search_abs_start : upstream_search_abs_end_excl]\n",
        "                for motif in sd_motifs_to_check:\n",
        "                    if motif in upstream_sequence_for_sd: # Простая проверка на вхождение\n",
        "                        has_sd_signal = True\n",
        "                        break\n",
        "        else: # Если SD мотивы не заданы, считаем, что фильтр пройден (или не применяется)\n",
        "            has_sd_signal = True\n",
        "\n",
        "        if not has_sd_signal:\n",
        "            continue # Не прошел SD фильтр, переходим к следующему ОРФу\n",
        "        orfs_passed_sd_count +=1\n",
        "\n",
        "        # 2. Фильтр по GC3-составу\n",
        "        orf_codons_for_gc3 = []\n",
        "        # Анализируем кодоны ОРФа, исключая стоп-кодон. Старт-кодон (ATG) включается.\n",
        "        for i in range(0, orf[\"length\"] - 3, 3): # -3 чтобы отрезать стоп-кодон\n",
        "            codon = orf[\"sequence\"][i:i+3]\n",
        "            orf_codons_for_gc3.append(codon)\n",
        "\n",
        "        if not orf_codons_for_gc3:\n",
        "            # Это может случиться, если ОРФ состоит только из старт-стоп (т.е. min_aa_length = 0)\n",
        "            # При min_aa_length=5, здесь всегда будут кодоны.\n",
        "            continue\n",
        "\n",
        "        orf_gc3_values = [1 if c[2] in 'GC' else 0 for c in orf_codons_for_gc3]\n",
        "        orf_mean_gc3 = statistics.mean(orf_gc3_values) # Должен быть хотя бы 1 кодон (старт)\n",
        "\n",
        "        gc3_passes_filter = False\n",
        "        if gc3_std_ref < 1e-6: # Если стандартное отклонение в референсе очень мало (или 0)\n",
        "            # Ожидаем почти точное совпадение со средним референсным\n",
        "            if abs(orf_mean_gc3 - gc3_mean_ref) < 0.01: # Допуск на небольшие отклонения\n",
        "                gc3_passes_filter = True\n",
        "        else:\n",
        "            lower_bound = gc3_mean_ref - gc3_std_dev_multiplier * gc3_std_ref\n",
        "            upper_bound = gc3_mean_ref + gc3_std_dev_multiplier * gc3_std_ref\n",
        "            if lower_bound <= orf_mean_gc3 <= upper_bound:\n",
        "                gc3_passes_filter = True\n",
        "\n",
        "        if not gc3_passes_filter:\n",
        "            continue # Не прошел GC3 фильтр\n",
        "\n",
        "        # Если ОРФ прошел все фильтры\n",
        "        filtered_orfs.append(orf)\n",
        "        orfs_passed_all_count +=1\n",
        "\n",
        "    print(f\"  ОРФов прошло SD фильтр: {orfs_passed_sd_count}\")\n",
        "    print(f\"  ОРФов прошло GC3 фильтр (из тех, что прошли SD): {orfs_passed_all_count}\") # Это и есть финальное число\n",
        "    print(f\"Расширенный поиск ОРФов завершен. {len(filtered_orfs)} ОРФов прошли все фильтры.\")\n",
        "    return filtered_orfs\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == '__main__':\n",
        "    # Создадим фиктивные файлы для примера\n",
        "    # Референсный геном (FASTA)\n",
        "    with open(\"reference_genome.fasta\", \"w\") as f:\n",
        "        f.write(\">ref_contig_1\\n\")\n",
        "        # Участок 1: AGGAGG NNNNNNN ATG GGC TTC TAA (SD в -15 до -10 отн. A из ATG -> A=pos 18)\n",
        "        #            012345678901234567890123456789\n",
        "        # CCAGGAGGCTATGCATGGGCCTTTTAACCCCGCTAG (Длина 38. ATG на 18 (0-based))\n",
        "        #  AGGAGG в [2:8]. ATG в [18:21]. Upstream окно для ATG[18]: [18-15 : 18-4] = [3:14] -> \"AGGCTATGCA\"\n",
        "        f.write(\"CCAGGAGGCTATGCATGGGCCTTTTAACCCCGCTAG\") # Ген1: ATG GGC TTC TAA (GC3 у GGC,TTC,TAA: C,C,A. Без стопа: C,C = 1.0)\n",
        "        # Участок 2: без явного SD ... ATG ... STOP ...\n",
        "        # ATTAAACCATGCCCGGGTAGCCCCGCTAGATTACG (Длина 35. ATG на 8)\n",
        "        # Upstream окно для ATG[8]: [8-15 : 8-4] = [-7 : 4] -> не подходит\n",
        "        f.write(\"ATTAAACCATGCCCGGGTAGCCCCGCTAGATTACG\")  # Ген2: ATG CCC GGG TAG (GC3 у CCC,GGG,TAG: C,G,G. Без стопа: C,G = 1.0)\n",
        "        # Участок 3: другой SD, другой GC3\n",
        "        # TTTAAGGAGTAAAATGAAACCCTGAAGGCCTTAA (Длина 34. ATG на 13)\n",
        "        #  AAGGAG в [3:9]. Upstream окно для ATG[13]: [13-15 : 13-4] = [-2:9] -> не подходит\n",
        "        f.write(\"TTTAAGGAGTAAAATGAAACCCTGAAGGCCTTAA\\n\") # Ген3: ATG AAA CCC TGA (GC3 у AAA,CCC,TGA: A,C,A. Без стопа: A,C = 0.5)\n",
        "\n",
        "    # Референсная аннотация (GFF)\n",
        "    with open(\"reference_annotation.gff\", \"w\") as f:\n",
        "        f.write(\"##gff-version 3\\n\")\n",
        "        # Ген 1: Координаты ATG GGC TTC TAA в \"CCAGGAGGCTATGCATGGGCCTTTTAACCCCGCTAG\"\n",
        "        # ATG начинается на 19-й позиции (1-based). Длина ОРФа 12.\n",
        "        f.write(\"ref_contig_1\\tRefSeq\\tCDS\\t19\\t30\\t.\\t+\\t0\\tID=cds1;Parent=gene1\\n\")\n",
        "        # Ген 2: Координаты ATG CCC GGG TAG в \"ATTAAACCATGCCCGGGTAGCCCCGCTAGATTACG\"\n",
        "        # ATG начинается на 9-й позиции (1-based). Длина ОРФа 12.\n",
        "        f.write(\"ref_contig_1\\tRefSeq\\tCDS\\t47\\t58\\t.\\t+\\t0\\tID=cds2;Parent=gene2\\n\") # Исправлены координаты относительно начала строки гена2 (38+9=47)\n",
        "        # Ген 3: Координаты ATG AAA CCC TGA в \"TTTAAGGAGTAAAATGAAACCCTGAAGGCCTTAA\"\n",
        "        # ATG начинается на 14-й позиции (1-based). Длина ОРФа 12.\n",
        "        f.write(\"ref_contig_1\\tRefSeq\\tCDS\\t87\\t98\\t.\\t+\\t0\\tID=cds3;Parent=gene3\\n\") # Исправлены координаты (38+35+14=87)\n",
        "\n",
        "\n",
        "    # Последовательность для анализа (длинная, чтобы было где искать)\n",
        "    # Используем конкатенацию тех же участков, чтобы проверить предсказания\n",
        "    dna_to_predict = (\n",
        "        \"XXXAGGAGGCTATGCATGGGCCTTTAAYYY\" + # ОРФ1, хороший SD\n",
        "        \"ZZZATTAAACCATGCCCGGGTAGTTT\" +   # ОРФ2, плохой SD\n",
        "        \"AAATTTTAAGGAGTAAAATGAAACCCTGATTT\" # ОРФ3, другой хороший SD\n",
        "        \"GGGAGGAGGAAAAATGAAATTCTAGCCC\" + # ОРФ4, хороший SD, короткий\n",
        "        \"AATGCCCTAA\"                     # ОРФ5, без SD, короткий\n",
        "    )\n",
        "\n",
        "\n",
        "    # 1. Обучаем параметры\n",
        "    candidate_sd_motifs_to_use = [\"AGGAGG\", \"AAGGAG\", \"GAGAGG\"]\n",
        "    sd_window = (-15, -4) # от -15 до -5 включительно, относительно A из ATG\n",
        "\n",
        "    try:\n",
        "        trained_params = train_orf_parameters(\n",
        "            \"reference_annotation.gff\",\n",
        "            \"reference_genome.fasta\",\n",
        "            candidate_sd_motifs_to_use,\n",
        "            sd_window\n",
        "        )\n",
        "\n",
        "        if trained_params: # Убедимся, что обучение прошло успешно\n",
        "            # 2. Ищем ОРФы на новой последовательности\n",
        "            final_orfs = find_orfs_enhanced(\n",
        "                dna_to_predict,\n",
        "                trained_params,\n",
        "                min_aa_length=3, # Поставим 3 для примера, чтобы короткие ОРФы тоже нашлись изначально\n",
        "                gc3_std_dev_multiplier=2.0\n",
        "            )\n",
        "\n",
        "            print(\"\\n--- Найденные и отфильтрованные ОРФы ---\")\n",
        "            if final_orfs:\n",
        "                for orf in final_orfs:\n",
        "                    print(f\"Старт: {orf['start_0based']}, Рамка: {orf['frame']}, \"\n",
        "                          f\"Длина: {orf['length']} п.н., Последовательность: {orf['sequence']}\")\n",
        "            else:\n",
        "                print(\"Отфильтрованных ОРФов не найдено.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Произошла общая ошибка в процессе выполнения: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k99Z_OR2umx4",
        "outputId": "5c01f554-7876-4ff1-9386-dad88dd28081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Запуск обучения параметров ОРФ из reference_annotation.gff и reference_genome.fasta...\n",
            "Парсинг GFF для извлечения CDS признаков...\n",
            "  Всего найдено CDS записей в GFF: 3\n",
            "  Пропущено CDS (нет ID контига в FASTA): 0\n",
            "  Пропущено CDS (не начинаются с ATG): 3\n",
            "  Пропущено CDS (длина не кратна 3): 0\n",
            "  Пропущено CDS (нет стандартного стоп-кодона): 0\n",
            "  Используется 0 валидных референсных CDS для обучения.\n",
            "Произошла ошибка: Не найдено валидных CDS в GFF для обучения. Проверьте GFF, FASTA и их соответствие.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def gc_content(seq):\n",
        "    seq = seq.upper()\n",
        "    g = seq.count('G')\n",
        "    c = seq.count('C')\n",
        "    return 100 * (g + c) / len(seq) if len(seq) > 0 else 0\n",
        "\n",
        "def compare_gc_by_type(tagged_pred):\n",
        "    tp_gc = []\n",
        "    fp_gc = []\n",
        "\n",
        "    for orf in tagged_pred:\n",
        "        gc = gc_content(orf['sequence'])\n",
        "        if orf['type'] == 'TP-full':\n",
        "            tp_gc.append(gc)\n",
        "        elif orf['type'] == 'FP':\n",
        "            fp_gc.append(gc)\n",
        "\n",
        "    # Средние значения\n",
        "    print(f\"\\nСредний GC-состав:\")\n",
        "    print(f\"TP-full: {sum(tp_gc)/len(tp_gc):.2f}% (n={len(tp_gc)})\")\n",
        "    print(f\"FP     : {sum(fp_gc)/len(fp_gc):.2f}% (n={len(fp_gc)})\")\n",
        "\n",
        "    # Гистограмма\n",
        "    plt.hist(tp_gc, bins=20, alpha=0.7, label='TP-full')\n",
        "    plt.hist(fp_gc, bins=20, alpha=0.7, label='FP')\n",
        "    plt.xlabel('GC-состав (%)')\n",
        "    plt.ylabel('Частота')\n",
        "    plt.title('Сравнение GC-состава: TP-full vs FP')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ETDwjFjTw2ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_gc_by_type(tagged_predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "pDiG0bGR0I8O",
        "outputId": "61fc12b2-88f8-4343-8d5f-503d1507084f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tagged_predicted' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-9fccad108589>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_gc_by_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tagged_predicted' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xCS_V4Xj0LHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2o9RcUolU97q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cemQZv1fTsGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "36ZQ6temT2mH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
